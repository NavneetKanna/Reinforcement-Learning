S F F F
F H F H
F F F H
H F F G

Reward schedule:
  - Reach goal(G): +1
  - Reach hole(H): 0
  - Reach frozen(F): 0

If out of bounds return same position

States = 16 (the number of positions the agent can be)
Actions = 4 (up, down, left, right)

Q table = (states, actions) = (16, 4)

First initialize it to zero 

[[0. 0. 0. 0.] -
 [0. 0. 0. 0.]  |
 [0. 0. 0. 0.]  | -> First row  
 [0. 0. 0. 0.] - 
 
 [0. 0. 0. 0.] -
 [0. 0. 0. 0.]  |
 [0. 0. 0. 0.]  | -> Second row
 [0. 0. 0. 0.] -
 
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]]

 Loop through episodes
    Loop through steps in each episodes
        n = random(0, 1)
        # Epsilon greedy action selection
        
        # Exploitation
        if n > epsilon
            action = Max of row of the current state (the highest q value, best action for that state)

        # Exploration
        else
            action = Choose random action from the state
        
        Execute the action on the current state which should return new state, reward and done (wheather it has reached the final goal)

        Update Q(s, a) 
        Q(s, a) = Q(s, a) + lr [R(s, a) + gamma * max Q(s', a') - Q(s, a)]

        where, 
        s, a = current state and action (not the new state)
        max Q(s', a') = max q value of the new state (new row)

        Update state to new state

        If done is true (which means it has reached the goal) go to the next episode

        




------------- Important ------------------

will explore the environment during the first episode of the game, rather than exploit it

