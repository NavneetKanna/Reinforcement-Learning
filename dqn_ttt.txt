create 2 nn (pred, target)

main.py

Fill the buffer by looping through episodes and steps 


Loop episodes:
    state = env.reset()

    while not done:
        Choose action using epsilon-greedy 
        next_state, reward, done, info = env.step(action)
        Store in relay buffer (state, action, next_state, reward, done)
        DQNagent.py

        At every interval copy the weights of pred model to target model

        state = next_state
        


DQNagent.py:

if len(relay buffer) < batchsize:
    return

states, actions, next_states, rewards, dones = pick 'n' samples from the memory that are selected uniformly at random, such that n = batchsize

q_pred = pred.forward(states)

q_target = target.forward(next_states).max(along the rows)

q_target[dones] = 0 # setting Q(s',a') to 0 when the current state is a terminal state
y_j = rewards + (self.discount * q_target)

loss = mse_loss(y_j, q_pred).mean()
loss.backward() # only for pred model