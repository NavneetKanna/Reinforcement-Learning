


Policy: A policy is a function that determines the action that 
        the agent should take in a given state.

Value function: It estimates the expected reward that the agent 
                will receive if it follows a particular policy.

Value function -> Expected reward for each an action.
Policy -> Choose action based on value function.

So, The value function helps the agent to evaluate different 
states and actions, and the policy uses this information to 
choose the action that is most likely to lead to the highest 
expected reward.


2 types of value function:

1) State-Value function:  Is the expected return when starting 
                          in state s and following pi thereafter.
For example, if you are playing a game of chess, the state value
function could represent the expected number of points you will 
earn if you reach a particular position on the board.

2) Action-value function: The value of taking action a in state 
                          s under a policy pi (or) the expected 
return starting from s, taking the action a, and thereafter
following policy pi.
For example, if you are playing a game of chess, the action value 
function could represent the expected number of points you will 
earn if you make a particular move from a particular position on 
the board.




On-Policy vs Off-Policy



Off-Policy: Policy used to take action is different than the    
            policy used for learning.
On-Policy: Policy used to take action is same as the policy used 
           for learning.

For example, 

Q-learning:

1) First, initialize the Q-function to a small random value for 
   all state-action pairs.

2) Then, for each episode:

3) Reset the environment to the starting state.

4) Choose an action according to an exploration policy 
   (e.g., epsilon-greedy).

5) Take the action and observe the reward and next state.

6) Update the Q-function according to the Q-learning update rule:
   Q(s, a) <- Q(s, a) + alpha * (r + gamma * max(Q(s', a')) - Q(s, a))
   where s and a are the current state and action, r is the reward, s' 
   and a' are the next state and action, alpha is the learning rate, and 
   gamma is the discount factor.

After completing multiple episodes, the Q-function will converge to the 
optimal action-value function, Q*(s, a). 
The optimal policy can then be derived by selecting the action with the 
highest Q-value in each state.

SARSA:

1) First, initialize the Q-function to a small random value for all 
   state-action pairs.

2) Then, for each episode:

3) Reset the environment to the starting state.

4) Choose an action according to an exploration policy 
   (e.g., epsilon-greedy).

5) Take the action and observe the reward and next state.

6) Choose the next action according to the current policy 
   (e.g., epsilon-greedy).

7) Update the Q-function according to the SARSA update rule:
   Q(s, a) <- Q(s, a) + alpha * (r + gamma * Q(s', a') - Q(s, a))
   where s and a are the current state and action, r is the reward, s' 
   and a' are the next state and action, alpha is the learning rate, and 
   gamma is the discount factor.

After completing multiple episodes, the Q-function will converge to the 
optimal action-value function, Q.


Q-function is is an off-policy algorithm because it is updated based on the 
action that the agent would have taken if it were following the optimal policy.
This allows Q-learning to learn from data that was generated by a different 
policy than the one being learned.


SARSA is an on-policy algorithm because it is updated using the same policy 
that is used to select actions.



In Q-learning and SARS, the policy that is being learned is the optimal policy, 
which is derived from the action-value function after learning is complete.
The optimal policy is the policy that maximizes the expected return, and is 
determined by selecting the action with the highest value in each state.
The optimal policy is the policy that is being learned, and it is determined by 
selecting the action with the highest value in each state.

On-Policy: The agent learns by itself without any aid of external agent.
Off-Policy: The agent learns by looking at humans or other agents.



Temporal Difference




Temporal difference (TD) learning is a type of reinforcement learning algorithm that uses
temporal difference errors to update the action-value function.

In TD learning, the agent estimates the value of each state and action by 
bootstrapping.

They can update the value function after each time step, rather than waiting 
for the episode to end.

In the SARSA update rule, the temporal difference (TD) error is computed as 
follows:

TD error = r + gamma * Q(s', a') - Q(s, a)

The TD error measures the difference between the expected return and the
current estimate of the action-value function for the state-action pair (s, a).
The expected return is computed using the reward r and the estimate of the 
action-value function for the next state-action pair (s', a'). The current 
estimate of the action-value function is the value of Q(s, a).

The update rule then adjusts the value of Q(s, a) in the direction of the TD 
error, with the learning rate alpha determining the size of the update.

Q(s, a) <- Q(s, a) + alpha * TD error

The agent is using the estimated value of the next state-action pair (s', a') 
to update the value of the current state-action pair (s, a). This process is 
known as bootstrapping.

This process is repeated at each time step, with the agent updating the 
action-value function based on the observed rewards and next states.